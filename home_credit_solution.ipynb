{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of folds for cross validation training\n",
    "\n",
    "kfold = 3\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open solution with different seeds https://github.com/neptune-ml/open-solution-home-credit\n",
    "\n",
    "# seed 0\n",
    "df_cv_nept_1 = pd.read_csv(r'open_solution_cv_1.csv')\n",
    "df_sub_nept_1 = pd.read_csv(r'open_solution_sub_1.csv')\n",
    "\n",
    "# seed 90210\n",
    "df_cv_nept_2 = pd.read_csv(r'open_solution_cv_2.csv')\n",
    "df_sub_nept_2 = pd.read_csv(r'open_solution_sub_2.csv')\n",
    "\n",
    "# kaggle solution https://www.kaggle.com/aantonova/797-lgbm-and-bayesian-optimization\n",
    "\n",
    "df_cv_kaggle = pd.read_csv(r'kaggle_cv.csv')\n",
    "df_sub_kaggle = pd.read_csv(r'kaggle_sub.csv')\n",
    "\n",
    "# my XGBoost model \n",
    "\n",
    "df_cv = pd.read_csv(r'model_cv.csv')\n",
    "df_sub = pd.read_csv(r'model_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = pd.merge(df_cv, df_cv_nept_1, how = 'left', on = ['SK_ID_CURR'])\n",
    "df_cv = pd.merge(df_cv, df_cv_nept_2, how = 'left', on = ['SK_ID_CURR'])\n",
    "df_cv = pd.merge(df_cv, df_cv_kaggle, how = 'left', on = ['SK_ID_CURR'])\n",
    "\n",
    "df_cv.columns = ['SK_ID_CURR', 'TARGET', 'MODEL', 'OPEN_SOLUTION_1', 'OPEN_SOLUTION_2', 'KAGGLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.merge(df_sub, df_sub_nept_1, how = 'left', on = ['SK_ID_CURR'])\n",
    "df_sub = pd.merge(df_sub, df_sub_nept_2, how = 'left', on = ['SK_ID_CURR'])\n",
    "df_sub = pd.merge(df_sub, df_sub_kaggle, how = 'left', on = ['SK_ID_CURR'])\n",
    "\n",
    "df_sub.columns = ['SK_ID_CURR', 'MODEL', 'OPEN_SOLUTION_1', 'OPEN_SOLUTION_2', 'KAGGLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df1, df2):\n",
    "    df2['TARGET'] = 999\n",
    "    cols = df1.columns\n",
    "    df_ = pd.concat([df1[cols], df2[cols]])\n",
    "    for col in df_.columns:\n",
    "        if col not in ['SK_ID_CURR', 'TARGET']:\n",
    "            df_[col] = df_[col].rank()\n",
    "            min_ = df_[col].min()\n",
    "            max_ = df_[col].max()\n",
    "            df_[col] = (df_[col] - min_ + 1e-7) / (max_ - min_)\n",
    "    return df_[df_['TARGET'] != 999].reset_index().drop('index', axis = 1), df_[df_['TARGET'] == 999].reset_index().drop('index', axis = 1)\n",
    "\n",
    "df, df_test = normalize(df_cv, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df.columns if c not in ['SK_ID_CURR', 'TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go from probabilities to odds\n",
    "\n",
    "for col in cols:\n",
    "    df[col] = np.log(df[col] / (1 - df[col]+1e-7))\n",
    "    df_test[col] = np.log(df_test[col] / (1 - df_test[col]+1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cols].values\n",
    "y = df['TARGET'].values\n",
    "\n",
    "y_pred = 0\n",
    "\n",
    "X_test = df_test[cols].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = 0*df['TARGET']\n",
    "\n",
    "n_rand = 1\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_val = X[test_index]\n",
    "    y_val = y[test_index]\n",
    "    clf = LogisticRegression(fit_intercept=True,  C = n_rand, random_state=n_rand)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('\\nFold {}:\\n'.format(str(i+1)))\n",
    "    print('AUC STACK: ', roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1]))\n",
    "    y_pred += clf.predict_proba(X_test)[:, 1] / (kfold*n_rand)\n",
    "\n",
    "    pred_train.loc[test_index] += clf.predict_proba(X_val)[:, 1] / (kfold*n_rand)\n",
    "\n",
    "print('\\nExpected AUC stack perfomance: ', roc_auc_score(y, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['TARGET'] = y_pred\n",
    "\n",
    "df_test['TARGET'] = df_test['TARGET'] / df_test['TARGET'].max()\n",
    "df_test[['SK_ID_CURR', 'TARGET']].to_csv(r'best_sub_085.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find customers with history of loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'train.csv')\n",
    "test = pd.read_csv(r'test.csv')\n",
    "\n",
    "tr = train[['SK_ID_CURR','DAYS_BIRTH',\n",
    "       'DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','CODE_GENDER','REGION_POPULATION_RELATIVE','TARGET']].copy()\n",
    "\n",
    "test[\"TARGET\"] = np.nan\n",
    "tst = test[['SK_ID_CURR','DAYS_BIRTH',\n",
    "       'DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','CODE_GENDER','REGION_POPULATION_RELATIVE','TARGET']].copy()\n",
    "\n",
    "tr['DAYS_REGISTRATION'] = tr['DAYS_REGISTRATION'] - tr['DAYS_BIRTH']\n",
    "tr['DAYS_ID_PUBLISH'] = tr['DAYS_ID_PUBLISH'] - tr['DAYS_BIRTH']\n",
    "\n",
    "tst['DAYS_REGISTRATION'] = tst['DAYS_REGISTRATION'] - tst['DAYS_BIRTH']\n",
    "tst['DAYS_ID_PUBLISH'] = tst['DAYS_ID_PUBLISH'] - tst['DAYS_BIRTH']\n",
    "\n",
    "data = pd.concat([tr, tst])\n",
    "\n",
    "d = data.groupby(['DAYS_REGISTRATION','DAYS_ID_PUBLISH',\n",
    "               'CODE_GENDER','REGION_POPULATION_RELATIVE'])['SK_ID_CURR'].count().sort_values(ascending = False)\n",
    "\n",
    "d = d[d > 1]\n",
    "\n",
    "d = d.reset_index()\n",
    "d.columns = ['DAYS_REGISTRATION','DAYS_ID_PUBLISH',\n",
    "               'CODE_GENDER','REGION_POPULATION_RELATIVE', 'n']\n",
    "\n",
    "d['cluster'] = d.index\n",
    "\n",
    "data = pd.merge(data, d, how = 'left', on = ['DAYS_REGISTRATION','DAYS_ID_PUBLISH','CODE_GENDER','REGION_POPULATION_RELATIVE'])\n",
    "\n",
    "\n",
    "\n",
    "data = data.sort_values(by = ['cluster', 'DAYS_BIRTH'])\n",
    "\n",
    "data['lag_target'] = data.groupby('cluster', sort = False)['TARGET'].shift(-1)\n",
    "data['lead_target'] = data.groupby('cluster', sort = False)['TARGET'].shift(1)\n",
    "\n",
    "data.loc[data['cluster'].isnull(), 'lag_target'] = -1\n",
    "data.loc[data['cluster'].isnull(), 'lead_target'] = -1\n",
    "\n",
    "# data.groupby('lead_target')['SK_ID_CURR'].count()\n",
    "\n",
    "# data.groupby('lead_target')['TARGET'].mean()\n",
    "\n",
    "# data.groupby('lag_target')['SK_ID_CURR'].count()\n",
    "\n",
    "# data.groupby('lag_target')['TARGET'].mean()\n",
    "\n",
    "\n",
    "\n",
    "special_ids = data[(data['lag_target'] == 1)]['SK_ID_CURR'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers who previosly had credit default have 82.8% chance for new default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_try = pd.read_csv(r'best_sub_085.csv')\n",
    "\n",
    "best_try.loc[best_try['SK_ID_CURR'].isin(special_ids), 'TARGET'] = best_try[best_try['SK_ID_CURR']\\\n",
    ".isin(special_ids)]['TARGET'].map(lambda x: np.random.choice([0,1], p = [1 - 0.828, 0.828]) + x)\n",
    "best_try.loc[best_try['TARGET'] > 1, 'TARGET'] = 1\n",
    "\n",
    "best_try.to_csv(r'best_sub_leaky.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
